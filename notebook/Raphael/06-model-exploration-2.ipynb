{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os, subprocess, requests, getpass, urllib.parse, sys, pathlib, string, spacy, bs4, \\\n",
    "numpy as np, seaborn as sns, pandas as pd, matplotlib.pyplot as plt, google.cloud.bigquery as bigquery, \\\n",
    "tensorflow as rf, bert as bc\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Specifying the Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "workingdir=os.getcwd()\n",
    "d=[d for d in os.listdir(workingdir)]\n",
    "n=0\n",
    "while not set(['notebook']).issubset(set(d)):\n",
    "    workingdir=str(pathlib.Path(workingdir).parents[0])\n",
    "\n",
    "    d=[d for d in os.listdir(str(workingdir))]\n",
    "    n+=1\n",
    "    if n>5:\n",
    "        break\n",
    "sys.path.insert(0, workingdir)\n",
    "os.chdir(workingdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Dealing with Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def decode_html(input_str: str, body: str = '') -> str:\n",
    "    soup = bs4.BeautifulSoup(input_str, 'html.parser')\n",
    "    \n",
    "    if len(body) == 0:\n",
    "        output = soup.text\n",
    "        return output\n",
    "    \n",
    "    html_elements = soup.find_all(body)\n",
    "    output = ' '.join(html_elements)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def nlp(input_list: list, spacy_obj) -> list:\n",
    "    doc = spacy_obj(input_list)\n",
    "    stopwords = list(string.punctuation + string.digits) + ['-pron-']\n",
    "    output = [token.lemma_.lower() for token in doc if not token.is_stop and token.lemma_.lower() not in stopwords]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def tokenizer(input_list: list) -> list:\n",
    "    spacy_obj = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    doc = spacy_obj(input_list)\n",
    "    stopwords = list(string.punctuation + string.digits) + ['-pron-']\n",
    "    output = [token.lemma_.lower() for token in doc if not token.is_stop and token.lemma_.lower() not in stopwords]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def preprocess(input_str: str) -> list:\n",
    "        spacy_object = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "        \n",
    "        try:\n",
    "            step_1 = decode_html(input_str)\n",
    "            step_2 = nlp(step_1, spacy_object)\n",
    "            return step_2\n",
    "        except TypeError:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def split_tags(tags: str) -> list:\n",
    "    return tags.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def create_label(tags, mapping):\n",
    "    for tag in tags:\n",
    "        if tag in mapping.keys():\n",
    "            return mapping[tag]\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Specifying GCP-Related Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axa-ch-machine-learning-dev\n"
     ]
    }
   ],
   "source": [
    "os.environ['PROJECT_ID'] = subprocess.run('gcloud config list project --format \"value(core.project)\"', \n",
    "                                          shell=True, check=True, stdout=subprocess.PIPE) \\\n",
    "                                            .stdout.decode().replace('\\n', '').replace('\\r', '')\n",
    "print(os.environ['PROJECT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ['BUCKET_NAME']='axa-ch-machine-learning-poc-dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tmp=os.environ['PROJECT_ID']\n",
    "except:\n",
    "    print('Env variable PROJECT not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['BUCKET_NAME']\n",
    "except:\n",
    "    print('Env variable BUCKET_NAME not defined!') \n",
    "    \n",
    "try:    \n",
    "    tmp=os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "except:\n",
    "    print('Env variable GOOGLE_APPLICATION_CREDENTIALS not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['REQUESTS_CA_BUNDLE']\n",
    "except:\n",
    "    print('Env variable REQUESTS_CA_BUNDLE not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['AXA_CH_CA_BUNDLE']\n",
    "except:\n",
    "    print('Env variable AXA_CA_CA_BUNDLE not defined!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Does the Connection Work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "use_proxy='Y'\n",
    "proxies = {\n",
    "    'https': os.environ['HTTPS_PROXY'],    \n",
    "    'http': os.environ['HTTP_PROXY']\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trying to access:https://www.google.com\n",
      "=>OK text/html; charset=ISO-8859-1\n",
      "\n",
      "trying to access:http://www.google.com\n",
      "=>OK text/html; charset=ISO-8859-1\n",
      "\n",
      "trying to access:https://www.example.com\n",
      "=>OK text/html; charset=UTF-8\n",
      "\n",
      "trying to access:http://www.example.com\n",
      "=>OK text/html; charset=UTF-8\n",
      "\n",
      "trying to access:https://github.com/j0hannes/cutter-ng\n",
      "=>OK text/html; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "list_url=['https://www.google.com',\n",
    "          'http://www.google.com',\n",
    "          'https://www.example.com',\n",
    "          'http://www.example.com',\n",
    "          'https://github.com/j0hannes/cutter-ng']\n",
    "\n",
    "for url in list_url:\n",
    "    print('')\n",
    "    print('trying to access:'+url)\n",
    "    try:\n",
    "        if use_proxy=='N':\n",
    "            r = requests.get(url)\n",
    "        else:\n",
    "            # SSL deactivated\n",
    "            #r = requests.get(url,proxies=proxies,verify=False)\n",
    "            r = requests.get(url,proxies=proxies,verify=True)\n",
    "            \n",
    "        if r.status_code == requests.codes.ok:\n",
    "            print('=>OK',r.headers['content-type'])\n",
    "        else:\n",
    "            # 407 Proxy Authentication Required\n",
    "            print ('=> ??', r.status_code)\n",
    "    except Exception as inst:\n",
    "        print('=>FAILED')\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT\n",
    " *\n",
    "FROM\n",
    "  `nlp_text_classification.stackoverflow_posts_complete`\n",
    "WHERE\n",
    "  tags <> ''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "df = client.query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df['tags_new'] = df['tags'].apply(split_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def make_subset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def classify(tags: list, subset: list) -> int:\n",
    "        for tag in tags:\n",
    "            if tag not in subset:\n",
    "                return 0\n",
    "        return 1\n",
    "    \n",
    "    subset = ['javascript', 'java', 'c#', 'php', 'python', 'android', 'jquery', 'html', 'c++', 'ios']\n",
    "    df['keep'] = df['tags_new'].apply(lambda tags: classify(tags, subset))\n",
    "    \n",
    "    return df[df['keep'] == 1][['id', 'title', 'body', 'tags', 'tags_new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_subset = make_subset(df)\n",
    "df_subset['body_new'] = df_subset['body'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TF-IDF and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x = df_subset['body_new'].apply(lambda array: ' '.join(array))\n",
    "y = MultiLabelBinarizer().fit_transform(df_subset['tags_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "estimators = [('tfidf', TfidfVectorizer()), ('clf', OneVsRestClassifier(RandomForestClassifier()))]\n",
    "parameters = {'tfidf__max_features': [1000, 2000], \n",
    "              'clf__estimator__n_estimators': [100],\n",
    "             'tfidf__ngram_range': [(1, 1), (2, 2), (3, 3)]}\n",
    "p = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(p, param_grid=parameters, cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C311723\\.conda\\envs\\env_nlp_text_class\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "score = grid.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_pred = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C311723\\.conda\\envs\\env_nlp_text_class\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\C311723\\.conda\\envs\\env_nlp_text_class\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.57      0.71        21\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       1.00      0.15      0.27        13\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00         3\n",
      "           5       0.83      0.31      0.45        16\n",
      "           6       0.76      0.82      0.79        34\n",
      "           7       0.88      0.61      0.72        23\n",
      "           8       0.91      0.56      0.69        18\n",
      "           9       1.00      0.36      0.53        11\n",
      "\n",
      "   micro avg       0.84      0.46      0.60       162\n",
      "   macro avg       0.63      0.34      0.42       162\n",
      "weighted avg       0.73      0.46      0.54       162\n",
      " samples avg       0.50      0.47      0.47       162\n",
      "\n",
      "0.3488372093023256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The model has a lot of problems that must be addressed:\n",
    "<ul>\n",
    "    <li>Too few samples</li>\n",
    "    <li>Class imbalance when spliting into training and evaluation dasets</li>\n",
    "    <li>Hyperparameter tuning</li>\n",
    "    <li>Feature selection (context is not considered) and weighting</li>\n",
    "</ul>        \n",
    "It is just a first evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\c311723\\.conda\\envs\\env_nlp_text_class\\lib\\site-packages (from bert-tensorflow) (1.12.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
    "BERT_VOCAB = 'data/uncased-l12-h768-a12/vocab.txt'\n",
    "BERT_INIT_CHKPNT = 'data/uncased-l12-h768-a12/bert_model.ckpt'\n",
    "BERT_CONFIG = 'data/uncased-l12-h768-a12/bert_config.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'bert' has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-9153b074586c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_tokenized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'bert' has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "vecs = bc.encode(texts2, is_tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-env_nlp_text_class]",
   "language": "python",
   "name": "conda-env-.conda-env_nlp_text_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
