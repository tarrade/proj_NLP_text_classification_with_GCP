{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# NLP Preprocessing using Beam/Dataflow\n",
    "## Setup Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "workingdir=os.getcwd()\n",
    "#print(workingdir)\n",
    "d=[d for d in os.listdir(workingdir)]\n",
    "n=0\n",
    "while not set(['notebook']).issubset(set(d)):\n",
    "    workingdir=str(pathlib.Path(workingdir).parents[0])\n",
    "    #print(workingdir)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    d=[d for d in os.listdir(str(workingdir))]\n",
    "    n+=1\n",
    "    if n>5:\n",
    "        break\n",
    "sys.path.insert(0, workingdir)\n",
    "os.chdir(workingdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import subprocess\n",
    "import datetime\n",
    "import subprocess, requests\n",
    "import apache_beam as beam\n",
    "from google.cloud import bigquery\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "import en_core_web_sm\n",
    "import bs4\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Defined GCP env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# get all variables here\n",
    "os.environ['PROJECT_ID'] =  subprocess.run('gcloud config list project --format \"value(core.project)\"',\n",
    "                                             shell=True, check=True,\n",
    "                                             stdout=subprocess.PIPE).stdout.decode().replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "os.environ['REGION'] = subprocess.run('gcloud config get-value compute/region  2> /dev/null',\n",
    "                                      shell=True, check=True,\n",
    "                                      stdout=subprocess.PIPE).stdout.decode().replace('\\n', '').replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env variable GOOGLE_APPLICATION_CREDENTIALS not defined!\n",
      "Env variable REQUESTS_CA_BUNDLE not defined!\n",
      "Env variable AXA_CA_CA_BUNDLE not defined!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tmp=os.environ['PROJECT_ID']\n",
    "except:\n",
    "    print('Env variable PROJECT not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['BUCKET_NAME']\n",
    "except:\n",
    "    print('Env variable BUCKET_NAME not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['REGION']\n",
    "except:\n",
    "    print('Env variable REGION not defined!') \n",
    "\n",
    "try:    \n",
    "    tmp=os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "except:\n",
    "    print('Env variable GOOGLE_APPLICATION_CREDENTIALS not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['REQUESTS_CA_BUNDLE']\n",
    "except:\n",
    "    print('Env variable REQUESTS_CA_BUNDLE not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['AXA_CH_CA_BUNDLE']\n",
    "except:\n",
    "    print('Env variable AXA_CA_CA_BUNDLE not defined!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Creating a DoFn Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class Split(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        self.id, self.title, self.body, self.tags = element.split(\",\")\n",
    "\n",
    "        return [{\n",
    "            'id': self.id,\n",
    "            'title': self.title,\n",
    "            'body': self.body,\n",
    "            'tags': self.tags\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class CleanText(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        self.spacy = en_core_web_sm.load()\n",
    "        \n",
    "    def __decode_html(self, input_str: str) -> str:\n",
    "        self.soup = bs4.BeautifulSoup(input_str, 'html.parser')\n",
    "        self.output = self.soup.text\n",
    "        return self.output\n",
    "\n",
    "    def __nlp(self, input_str: str) -> list:\n",
    "        self.doc = self.spacy(input_str)\n",
    "        self.stopwords = list(string.punctuation + string.digits) + ['-pron-']\n",
    "        self.output = [token.lemma_.lower() for token in self.doc if not token.is_stop \n",
    "                  and token.lemma_.lower() not in self.stopwords]\n",
    "        return self.output\n",
    "\n",
    "    def __split_tags(self, tags: str) -> list:\n",
    "        return tags.split('|')\n",
    "\n",
    "    def process(self, element):\n",
    "        self.title_array = self.__nlp(element['title'])\n",
    "        self.body_decoded = self.__decode_html(element['tags'])\n",
    "        self.body_array = self.__nlp(self.body_decoded)\n",
    "        self.tag_array = self.__split_tags(element['tags'])\n",
    "        \n",
    "        return [{'id': int(element['id']), \n",
    "                 'title': self.title_array, \n",
    "                 'body': self.body_array, \n",
    "                 'tags': self.tag_array}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# define query table\n",
    "def create_query():\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      id,\n",
    "      title,\n",
    "      body,\n",
    "      tags\n",
    "    FROM\n",
    "      `bigquery-public-data.stackoverflow.stackoverflow_posts`\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "table_schema = {'fields': [\n",
    "    {'name': 'id', 'type': 'NUMERIC', 'mode': 'REQUIRED'},\n",
    "    {'name': 'title', 'type': 'ARRAY', 'mode': 'NULLABLE'},\n",
    "    {'name': 'body', 'type': 'ARRAY', 'mode': 'NULLABLE'},\n",
    "    {'name': 'tags', 'type': 'ARRAY', 'mode': 'NULLABLE'},\n",
    "]}\n",
    "new_table = 'nlp_text_classification.stackoverflow_posts_preprocessed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preprocessing using Beam/Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        -RUNNER: \"DirectRunner\" or \"DataflowRunner\". Specfy to run the pipeline locally or on Google Cloud respectively.\n",
    "    Side-effects:\n",
    "        -Creates and executes dataflow pipeline.\n",
    "        See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "    \"\"\"\n",
    "    job_name = 'test-stackoverflow' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    project = os.environ['PROJECT_ID']\n",
    "    region = os.environ['REGION']\n",
    "    output_dir = \"gs://{0}/stackoverflow/\".format(os.environ['BUCKET_NAME'])\n",
    "    local_file = 'data/beam_test.csv'\n",
    "\n",
    "    # options    \n",
    "    options = PipelineOptions()\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project =  project\n",
    "    google_cloud_options.job_name =  job_name\n",
    "    google_cloud_options.region = region\n",
    "    google_cloud_options.staging_location = os.path.join(output_dir, 'tmp', 'staging')\n",
    "    google_cloud_options.temp_location = os.path.join(output_dir, 'tmp')\n",
    "    # done by command line\n",
    "    options.view_as(StandardOptions).runner = 'DataflowRunner'\n",
    "\n",
    "    # instantantiate Pipeline object using PipelineOptions\n",
    "    print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "\n",
    "    p = beam.Pipeline(options=options)    \n",
    "    table = p | 'Read from BigQuery' >> beam.io.Read(beam.io.BigQuerySource(\n",
    "        # query\n",
    "        query=create_query(),\n",
    "        # use standard SQL for the above query\n",
    "        use_standard_sql=True))\n",
    "    clean_text = table | 'Clean Text' >> beam.ParDo(CleanText())\n",
    "    #clean_text | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
    "    #    # The table name is a required argument for the BigQuery\n",
    "    #    table='test_stackoverflow_pp',\n",
    "    #    dataset='test',\n",
    "    #    project=project,\n",
    "    #    # Here we use the JSON schema read in from a JSON file.\n",
    "    #    # Specifying the schema allows the API to create the table correctly if it does not yet exist.\n",
    "    #    schema=table_schema,\n",
    "    #    # Creates the table in BigQuery if it does not yet exist.\n",
    "    #    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "    #    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "    \n",
    "    #output = p | 'Read from BigQuery' >> beam.io.Read(beam.io.BigQuerySource(\n",
    "    #    # query\n",
    "    #    query=create_query(),\n",
    "    #    # use standard SQL for the above query\n",
    "    #    use_standard_sql=True)\n",
    "    #    )\n",
    "    #output | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
    "    #    # The table name is a required argument for the BigQuery\n",
    "    #    table='test_stackoverflow_beam',\n",
    "    #    dataset='test',\n",
    "    #    project=project,\n",
    "    #    # Here we use the JSON schema read in from a JSON file.\n",
    "    #    # Specifying the schema allows the API to create the table correctly if it does not yet exist.\n",
    "    #    schema=table_schema,\n",
    "    #    # Creates the table in BigQuery if it does not yet exist.\n",
    "    #    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "    #    # Deletes all data in the BigQuery table before writing.\n",
    "    #    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n",
    "    #    # not needed, from with clause\n",
    "\n",
    "    if options.view_as(StandardOptions).runner == 'DataflowRunner':\n",
    "        print('DataflowRunner')\n",
    "        p.run()\n",
    "    else:\n",
    "        print('Default: DirectRunner')\n",
    "        result = p.run()\n",
    "        result.wait_until_finish()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main process ...\n",
      "Launching Dataflow job test-stackoverflow-191104-121455 ... hang on\n",
      "DataflowRunner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting GCS upload to gs://nlp-text-classification/stackoverflow/tmp/staging/test-stackoverflow-191104-121455.1572869729.334275/pipeline.pb...\n",
      "INFO:root:Completed GCS upload to gs://nlp-text-classification/stackoverflow/tmp/staging/test-stackoverflow-191104-121455.1572869729.334275/pipeline.pb in 2 seconds.\n",
      "INFO:root:Downloading source distribution of the SDK from PyPi\n",
      "INFO:root:Executing command: ['/home/.conda-env/env_nlp_text_class/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp70msl3z6', 'apache-beam==2.16.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:root:Staging SDK sources from PyPI to gs://nlp-text-classification/stackoverflow/tmp/staging/test-stackoverflow-191104-121455.1572869729.334275/dataflow_python_sdk.tar\n",
      "INFO:root:Starting GCS upload to gs://nlp-text-classification/stackoverflow/tmp/staging/test-stackoverflow-191104-121455.1572869729.334275/dataflow_python_sdk.tar...\n",
      "INFO:root:Completed GCS upload to gs://nlp-text-classification/stackoverflow/tmp/staging/test-stackoverflow-191104-121455.1572869729.334275/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:root:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:root:Executing command: ['/home/.conda-env/env_nlp_text_class/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp70msl3z6', 'apache-beam==2.16.0', '--no-deps', '--only-binary', ':all:', '--python-version', '36', '--implementation', 'cp', '--abi', 'cp36m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:root:Staging binary distribution of the SDK from PyPI to gs://nlp-text-classification/stackoverflow/tmp/staging/test-stackoverflow-191104-121455.1572869729.334275/apache_beam-2.16.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "INFO:root:Starting GCS upload to gs://nlp-text-classification/stackoverflow/tmp/staging/test-stackoverflow-191104-121455.1572869729.334275/apache_beam-2.16.0-cp36-cp36m-manylinux1_x86_64.whl...\n",
      "INFO:root:Completed GCS upload to gs://nlp-text-classification/stackoverflow/tmp/staging/test-stackoverflow-191104-121455.1572869729.334275/apache_beam-2.16.0-cp36-cp36m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:root:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-0396aa22-70a8-456a-bd63-fb5484b650a6.json']\n",
      "WARNING:root:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-0396aa22-70a8-456a-bd63-fb5484b650a6.json']\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "DEBUG:root:Caught socket error, retrying: [Errno 32] Broken pipe\n",
      "DEBUG:root:Retrying request to url https://dataflow.googleapis.com/v1b3/projects/nlp-text-classification/locations//jobs?alt=json after exception [Errno 32] Broken pipe\n",
      "DEBUG:root:Caught socket error, retrying: [Errno 32] Broken pipe\n",
      "DEBUG:root:Retrying request to url https://dataflow.googleapis.com/v1b3/projects/nlp-text-classification/locations//jobs?alt=json after exception [Errno 32] Broken pipe\n",
      "DEBUG:root:Caught socket error, retrying: [Errno 32] Broken pipe\n",
      "DEBUG:root:Retrying request to url https://dataflow.googleapis.com/v1b3/projects/nlp-text-classification/locations//jobs?alt=json after exception [Errno 32] Broken pipe\n",
      "DEBUG:root:Caught socket error, retrying: [Errno 32] Broken pipe\n",
      "DEBUG:root:Retrying request to url https://dataflow.googleapis.com/v1b3/projects/nlp-text-classification/locations//jobs?alt=json after exception [Errno 32] Broken pipe\n",
      "WARNING:root:Retry with exponential backoff: waiting for 3.6935465316390035 seconds before retrying submit_job_description because we caught exception: BrokenPipeError: [Errno 32] Broken pipe\n",
      " Traceback for above exception (most recent call last):\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/utils/retry.py\", line 206, in wrapper\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\", line 593, in submit_job_description\n",
      "    response = self._client.projects_locations_jobs.Create(request)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py\", line 657, in Create\n",
      "    config, request, global_params=global_params)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apitools/base/py/base_api.py\", line 729, in _RunMethod\n",
      "    http, http_request, **opts)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apitools/base/py/http_wrapper.py\", line 346, in MakeRequest\n",
      "    check_response_func=check_response_func)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apitools/base/py/http_wrapper.py\", line 396, in _MakeRequestNoRetry\n",
      "    redirections=redirections, connection_type=connection_type)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/oauth2client/transport.py\", line 169, in new_request\n",
      "    redirections, connection_type)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/oauth2client/transport.py\", line 169, in new_request\n",
      "    redirections, connection_type)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/httplib2/__init__.py\", line 1924, in request\n",
      "    cachekey,\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/httplib2/__init__.py\", line 1595, in _request\n",
      "    conn, request_uri, method, body, headers\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/httplib2/__init__.py\", line 1502, in _conn_request\n",
      "    conn.request(method, request_uri, body, headers)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/http/client.py\", line 1254, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/http/client.py\", line 1300, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/http/client.py\", line 1249, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/http/client.py\", line 1075, in _send_output\n",
      "    self.send(chunk)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/http/client.py\", line 996, in send\n",
      "    self.sock.sendall(data)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/ssl.py\", line 975, in sendall\n",
      "    v = self.send(byte_view[count:])\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/ssl.py\", line 944, in send\n",
      "    return self._sslobj.write(data)\n",
      "  File \"/home/.conda-env/env_nlp_text_class/lib/python3.6/ssl.py\", line 642, in write\n",
      "    return self._sslobj.write(data)\n",
      "\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "HttpError accessing <https://dataflow.googleapis.com/v1b3/projects/nlp-text-classification/locations//jobs?alt=json>: response: <{'content-type': 'text/html; charset=UTF-8', 'referrer-policy': 'no-referrer', 'content-length': '2393', 'date': 'Mon, 04 Nov 2019 12:16:14 GMT', 'connection': 'close', 'status': '413'}>, content <<!DOCTYPE html>\n<html lang=en>\n  <meta charset=utf-8>\n  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\n  <title>Error 413 (Request Entity Too Large)!!1</title>\n  <style>\n    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  </style>\n  <a href=//www.google.com/><span id=logo aria-label=Google></span></a>\n  <p><b>413.</b> <ins>That���s an error.</ins>\n  <p>Your client issued a request that was too large.\n <script>\n  (function() { var c=function(a,d,b){a=a+\"=deleted; path=\"+d;null!=b&&(a+=\"; domain=\"+b);document.cookie=a+\"; expires=Thu, 01 Jan 1970 00:00:00 GMT\"};var g=function(a){var d=e,b=location.hostname;c(d,a,null);c(d,a,b);for(var f=0;;){f=b.indexOf(\".\",f+1);if(0>f)break;c(d,a,b.substring(f+1))}};var h;if(4E3<unescape(encodeURI(document.cookie)).length){for(var k=document.cookie.split(\";\"),l=[],m=0;m<k.length;m++){var n=k[m].match(/^\\s*([^=]+)/);n&&l.push(n[1])}for(var p=0;p<l.length;p++){var e=l[p];g(\"/\");for(var q=location.pathname,r=0;;){r=q.indexOf(\"/\",r+1);if(0>r)break;var t=q.substring(0,r);g(t);g(t+\"/\")}\"/\"!=q.charAt(q.length-1)&&(g(q),g(q+\"/\"))}h=!0}else h=!1;\nh&&setTimeout(function(){if(history.replaceState){var a=location.href;history.replaceState(null,\"\",\"/\");location.replace(a)}},1E3); })();\n\n</script>\n <ins>That���s all we know.</ins>\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-8f37ebeb1ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting main process ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-b7cbc82215d0>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataflowRunner'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataflowRunner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Default: DirectRunner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    405\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m           self._options).run(False)\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_type_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    418\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;31m# raise an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     result = DataflowPipelineResult(\n\u001b[0;32m--> 485\u001b[0;31m         self.dataflow_client.create_job(self.job), self)\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;31m# TODO(BEAM-4274): Circular import runners-metrics. Requires refactoring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/utils/retry.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36mcreate_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtemplate_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     logging.info('A template was just created at location %s',\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/utils/retry.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36msubmit_job_description\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojects_locations_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadStatusCodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       logging.error('HTTP status %d trying to create job'\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py\u001b[0m in \u001b[0;36mCreate\u001b[0;34m(self, request, global_params)\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetMethodConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Create'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m       return self._RunMethod(\n\u001b[0;32m--> 657\u001b[0;31m           config, request, global_params=global_params)\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     Create.method_config = lambda: base_api.ApiMethodInfo(\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apitools/base/py/base_api.py\u001b[0m in \u001b[0;36m_RunMethod\u001b[0;34m(self, method_config, request, global_params, upload, upload_config, download)\u001b[0m\n\u001b[1;32m    729\u001b[0m                 http, http_request, **opts)\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessHttpResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mProcessHttpResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apitools/base/py/base_api.py\u001b[0m in \u001b[0;36mProcessHttpResponse\u001b[0;34m(self, method_config, http_response, request)\u001b[0m\n\u001b[1;32m    735\u001b[0m         return self.__client.ProcessResponse(\n\u001b[1;32m    736\u001b[0m             \u001b[0mmethod_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             self.__ProcessHttpResponse(method_config, http_response, request))\n\u001b[0m",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apitools/base/py/base_api.py\u001b[0m in \u001b[0;36m__ProcessHttpResponse\u001b[0;34m(self, method_config, http_response, request)\u001b[0m\n\u001b[1;32m    602\u001b[0m                                              http_client.NO_CONTENT):\n\u001b[1;32m    603\u001b[0m             raise exceptions.HttpError.FromResponse(\n\u001b[0;32m--> 604\u001b[0;31m                 http_response, method_config=method_config, request=request)\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNO_CONTENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;31m# TODO(craigcitro): Find out why _replace doesn't seem to work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: HttpError accessing <https://dataflow.googleapis.com/v1b3/projects/nlp-text-classification/locations//jobs?alt=json>: response: <{'content-type': 'text/html; charset=UTF-8', 'referrer-policy': 'no-referrer', 'content-length': '2393', 'date': 'Mon, 04 Nov 2019 12:16:14 GMT', 'connection': 'close', 'status': '413'}>, content <<!DOCTYPE html>\n<html lang=en>\n  <meta charset=utf-8>\n  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\n  <title>Error 413 (Request Entity Too Large)!!1</title>\n  <style>\n    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  </style>\n  <a href=//www.google.com/><span id=logo aria-label=Google></span></a>\n  <p><b>413.</b> <ins>That���s an error.</ins>\n  <p>Your client issued a request that was too large.\n <script>\n  (function() { var c=function(a,d,b){a=a+\"=deleted; path=\"+d;null!=b&&(a+=\"; domain=\"+b);document.cookie=a+\"; expires=Thu, 01 Jan 1970 00:00:00 GMT\"};var g=function(a){var d=e,b=location.hostname;c(d,a,null);c(d,a,b);for(var f=0;;){f=b.indexOf(\".\",f+1);if(0>f)break;c(d,a,b.substring(f+1))}};var h;if(4E3<unescape(encodeURI(document.cookie)).length){for(var k=document.cookie.split(\";\"),l=[],m=0;m<k.length;m++){var n=k[m].match(/^\\s*([^=]+)/);n&&l.push(n[1])}for(var p=0;p<l.length;p++){var e=l[p];g(\"/\");for(var q=location.pathname,r=0;;){r=q.indexOf(\"/\",r+1);if(0>r)break;var t=q.substring(0,r);g(t);g(t+\"/\")}\"/\"!=q.charAt(q.length-1)&&(g(q),g(q+\"/\"))}h=!0}else h=!1;\nh&&setTimeout(function(){if(history.replaceState){var a=location.href;history.replaceState(null,\"\",\"/\");location.replace(a)}},1E3); })();\n\n</script>\n <ins>That���s all we know.</ins>\n>"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #logging.getLogger().setLevel(logging.DEBUG)\n",
    "    logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "    print('Starting main process ...')\n",
    "    preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Apache Beam and GCP Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from setuptools import find_packages\n",
    "find_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline(argv=sys.argv) as p:\n",
    "    file = p                  | \"ReadLocalFile\" >> beam.io.ReadFromText(local_file)\n",
    "    table = file              | \"CreateDictionary\"  >> beam.ParDo(Split())\n",
    "    clean_text = table        | \"ProcessFields\" >> beam.ParDo(CleanText())\n",
    "    clean_text                | \"WriteLocalFile\" >> beam.io.WriteToText('data/beam_output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## GCP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "query = '''SELECT\n",
    "  id,\n",
    "  title,\n",
    "  body,\n",
    "  tags\n",
    "FROM\n",
    "  bigquery-public-data:stackoverflow.stackoverflow_posts'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "table_schema = {'fields': [\n",
    "    {'name': 'id', 'type': 'NUMERIC', 'mode': 'REQUIRED'},\n",
    "    {'name': 'title', 'type': 'ARRAY', 'mode': 'NULLABLE'},\n",
    "    {'name': 'body', 'type': 'ARRAY', 'mode': 'NULLABLE'},\n",
    "    {'name': 'tags', 'type': 'ARRAY', 'mode': 'NULLABLE'},\n",
    "]}\n",
    "new_table = 'nlp_text_classification.stackoverflow_posts_preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline(options=pipeline_options) as p:\n",
    "    table = p                 | \"QueryTable\" >> beam.io.Read(beam.io.BigQuerySource(query))\n",
    "    clean_text = table        | \"ProcessFields\" >> beam.ParDo(CleanText())\n",
    "    clean_text                | \"WriteTable\" >> beam.io.WriteToBigQuery(\n",
    "                                                    new_table,\n",
    "                                                    schema=table_schema,\n",
    "                                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_nlp_text_class]",
   "language": "python",
   "name": "conda-env-env_nlp_text_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
