{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Preprocessing using Beam/Dataflow\n",
    "## Setup Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "workingdir=os.getcwd()\n",
    "#print(workingdir)\n",
    "d=[d for d in os.listdir(workingdir)]\n",
    "n=0\n",
    "while not set(['notebook']).issubset(set(d)):\n",
    "    workingdir=str(pathlib.Path(workingdir).parents[0])\n",
    "    #print(workingdir)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    d=[d for d in os.listdir(str(workingdir))]\n",
    "    n+=1\n",
    "    if n>5:\n",
    "        break\n",
    "sys.path.insert(0, workingdir)\n",
    "os.chdir(workingdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import subprocess\n",
    "import datetime\n",
    "import subprocess, requests\n",
    "import apache_beam as beam\n",
    "from google.cloud import bigquery\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Defined GCP env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# get all variables here\n",
    "os.environ['PROJECT_ID'] =  subprocess.run('gcloud config list project --format \"value(core.project)\"',\n",
    "                                             shell=True, check=True,\n",
    "                                             stdout=subprocess.PIPE).stdout.decode().replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "os.environ['REGION'] = subprocess.run('gcloud config get-value compute/region  2> /dev/null',\n",
    "                                      shell=True, check=True,\n",
    "                                      stdout=subprocess.PIPE).stdout.decode().replace('\\n', '').replace('\\r', '')\n",
    "# bucket storage name\n",
    "os.environ['BUCKET_NAME']='axa-ch-machine-learning-poc-dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tmp=os.environ['PROJECT_ID']\n",
    "except:\n",
    "    print('Env variable PROJECT not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['BUCKET_NAME']\n",
    "except:\n",
    "    print('Env variable BUCKET_NAME not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['REGION']\n",
    "except:\n",
    "    print('Env variable REGION not defined!') \n",
    "\n",
    "try:    \n",
    "    tmp=os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "except:\n",
    "    print('Env variable GOOGLE_APPLICATION_CREDENTIALS not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['REQUESTS_CA_BUNDLE']\n",
    "except:\n",
    "    print('Env variable REQUESTS_CA_BUNDLE not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['AXA_CH_CA_BUNDLE']\n",
    "except:\n",
    "    print('Env variable AXA_CA_CA_BUNDLE not defined!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preprocessing using Beam/Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define a query to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# define query table\n",
    "def create_query():\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "    *\n",
    "    FROM\n",
    "    `bigquery-public-data.stackoverflow.tags`\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Get the schema of the input table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting schema\n",
    "! bq show --format=prettyjson bigquery-public-data:stackoverflow.tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "table_schema = {\n",
    "    'fields': [\n",
    "        {\n",
    "            'mode': 'NULLABLE',\n",
    "            'name': 'id',\n",
    "            'type': 'INTEGER'\n",
    "        },\n",
    "        {\n",
    "            'mode': 'NULLABLE',\n",
    "            'name': 'tag_name',\n",
    "            'type': 'STRING'\n",
    "        },\n",
    "        {\n",
    "            'mode': 'NULLABLE',\n",
    "            'name': 'count',\n",
    "            'type': 'INTEGER'\n",
    "        },\n",
    "        {\n",
    "            'mode': 'NULLABLE',\n",
    "            'name': 'excerpt_post_id',\n",
    "            'type': 'INTEGER'\n",
    "        },\n",
    "        {\n",
    "            'mode': 'NULLABLE',\n",
    "            'name': 'wiki_post_id',\n",
    "            'type': 'INTEGER'\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        -RUNNER: \"DirectRunner\" or \"DataflowRunner\". Specfy to run the pipeline locally or on Google Cloud respectively.\n",
    "    Side-effects:\n",
    "        -Creates and executes dataflow pipeline.\n",
    "        See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "    \"\"\"\n",
    "    job_name = 'test-stackoverflow' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    project = os.environ['PROJECT_ID']\n",
    "    region = os.environ['REGION']\n",
    "    output_dir = \"gs://{0}/stackoverflow/\".format(os.environ['BUCKET_NAME'])\n",
    "\n",
    "    # options\n",
    "    options = PipelineOptions()\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project =  project\n",
    "    google_cloud_options.job_name =  job_name\n",
    "    google_cloud_options.region = region\n",
    "    google_cloud_options.staging_location = os.path.join(output_dir, 'tmp', 'staging')\n",
    "    google_cloud_options.temp_location = os.path.join(output_dir, 'tmp')\n",
    "    # done by command line\n",
    "    options.view_as(StandardOptions).runner = 'DataflowRunner'\n",
    "\n",
    "    # instantantiate Pipeline object using PipelineOptions\n",
    "    print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "\n",
    "    p = beam.Pipeline(options=options)\n",
    "    output = p | 'Read from BigQuery' >> beam.io.Read(beam.io.BigQuerySource(\n",
    "        # query\n",
    "        query=create_query(),\n",
    "        # use standard SQL for the above query\n",
    "        use_standard_sql=True)\n",
    "        )\n",
    "    output | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
    "        # The table name is a required argument for the BigQuery\n",
    "        table='test_stackoverflow_beam',\n",
    "        dataset='test',\n",
    "        project=project,\n",
    "        # Here we use the JSON schema read in from a JSON file.\n",
    "        # Specifying the schema allows the API to create the table correctly if it does not yet exist.\n",
    "        schema=table_schema,\n",
    "        # Creates the table in BigQuery if it does not yet exist.\n",
    "        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "        # Deletes all data in the BigQuery table before writing.\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n",
    "        # not needed, from with clause\n",
    "\n",
    "    if options.view_as(StandardOptions).runner == 'DataflowRunner':\n",
    "        print('DataflowRunner')\n",
    "        p.run()\n",
    "    else:\n",
    "        print('Default: DirectRunner')\n",
    "        result = p.run()\n",
    "        result.wait_until_finish()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "    print('Starting main process ...')\n",
    "    preprocess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_nlp_text_class]",
   "language": "python",
   "name": "conda-env-env_nlp_text_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
