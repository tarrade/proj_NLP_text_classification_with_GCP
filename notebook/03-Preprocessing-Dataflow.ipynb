{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Preprocessing using Beam/Dataflow\n",
    "## Setup Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "workingdir=os.getcwd()\n",
    "#print(workingdir)\n",
    "d=[d for d in os.listdir(workingdir)]\n",
    "n=0\n",
    "while not set(['notebook']).issubset(set(d)):\n",
    "    workingdir=str(pathlib.Path(workingdir).parents[0])\n",
    "    #print(workingdir)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    d=[d for d in os.listdir(str(workingdir))]\n",
    "    n+=1\n",
    "    if n>5:\n",
    "        break\n",
    "sys.path.insert(0, workingdir)\n",
    "os.chdir(workingdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Import the client library\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "import apache_beam as beam\n",
    "import datetime\n",
    "import subprocess, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Defined GCP env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axa-ch-machine-learning-dev\n"
     ]
    }
   ],
   "source": [
    "os.environ['PROJECT_ID'] = subprocess.run('gcloud config list project --format \"value(core.project)\"', shell=True, check=True, stdout=subprocess.PIPE).stdout.decode().replace('\\n', '').replace('\\r', '')\n",
    "print(os.environ['PROJECT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# bucket storage name\n",
    "os.environ['BUCKET_NAME']='axa-ch-machine-learning-poc-dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tmp=os.environ['PROJECT_ID']\n",
    "except:\n",
    "    print('Env variable PROJECT not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['BUCKET_NAME']\n",
    "except:\n",
    "    print('Env variable BUCKET_NAME not defined!') \n",
    "    \n",
    "try:    \n",
    "    tmp=os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "except:\n",
    "    print('Env variable GOOGLE_APPLICATION_CREDENTIALS not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['REQUESTS_CA_BUNDLE']\n",
    "except:\n",
    "    print('Env variable REQUESTS_CA_BUNDLE not defined!') \n",
    "\n",
    "try:\n",
    "    tmp=os.environ['AXA_CH_CA_BUNDLE']\n",
    "except:\n",
    "    print('Env variable AXA_CA_CA_BUNDLE not defined!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Testing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# use env variables\n",
    "use_proxy='Y'\n",
    "proxies = {\n",
    "    'https': os.environ['HTTPS_PROXY'],    \n",
    "    'http': os.environ['HTTP_PROXY']\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trying to access:https://www.google.com\n",
      "=>OK text/html; charset=ISO-8859-1\n",
      "\n",
      "trying to access:http://www.google.com\n",
      "=>OK text/html; charset=ISO-8859-1\n",
      "\n",
      "trying to access:https://www.example.com\n",
      "=>OK text/html; charset=UTF-8\n",
      "\n",
      "trying to access:http://www.example.com\n",
      "=>OK text/html; charset=UTF-8\n",
      "\n",
      "trying to access:https://github.com/j0hannes/cutter-ng\n",
      "=>OK text/html; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "list_url=['https://www.google.com',\n",
    "          'http://www.google.com',\n",
    "          'https://www.example.com',\n",
    "          'http://www.example.com',\n",
    "          'https://github.com/j0hannes/cutter-ng']\n",
    "\n",
    "for url in list_url:\n",
    "    print('')\n",
    "    print('trying to access:'+url)\n",
    "    try:\n",
    "        if use_proxy=='N':\n",
    "            r = requests.get(url)\n",
    "        else:\n",
    "            # SSL deactivated\n",
    "            #r = requests.get(url,proxies=proxies,verify=False)\n",
    "            r = requests.get(url,proxies=proxies,verify=True)\n",
    "            \n",
    "        if r.status_code == requests.codes.ok:\n",
    "            print('=>OK',r.headers['content-type'])\n",
    "        else:\n",
    "            # 407 Proxy Authentication Required\n",
    "            print ('=> ??', r.status_code)\n",
    "    except Exception as inst:\n",
    "        print('=>FAILED')\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Extracting some data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_catalog</th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>field_path</th>\n",
       "      <th>data_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>comment_count</td>\n",
       "      <td>comment_count</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>post_type_id</td>\n",
       "      <td>post_type_id</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>parent_id</td>\n",
       "      <td>parent_id</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>view_count</td>\n",
       "      <td>view_count</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>answer_count</td>\n",
       "      <td>answer_count</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>owner_user_id</td>\n",
       "      <td>owner_user_id</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>id</td>\n",
       "      <td>id</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>score</td>\n",
       "      <td>score</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>accepted_answer_id</td>\n",
       "      <td>accepted_answer_id</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>favorite_count</td>\n",
       "      <td>favorite_count</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>last_editor_user_id</td>\n",
       "      <td>last_editor_user_id</td>\n",
       "      <td>INT64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>last_editor_display_name</td>\n",
       "      <td>last_editor_display_name</td>\n",
       "      <td>STRING</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>owner_display_name</td>\n",
       "      <td>owner_display_name</td>\n",
       "      <td>STRING</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>body</td>\n",
       "      <td>body</td>\n",
       "      <td>STRING</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>tags</td>\n",
       "      <td>tags</td>\n",
       "      <td>STRING</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>title</td>\n",
       "      <td>title</td>\n",
       "      <td>STRING</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>community_owned_date</td>\n",
       "      <td>community_owned_date</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>last_activity_date</td>\n",
       "      <td>last_activity_date</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>last_edit_date</td>\n",
       "      <td>last_edit_date</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>stackoverflow_posts</td>\n",
       "      <td>creation_date</td>\n",
       "      <td>creation_date</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           table_catalog   table_schema           table_name  \\\n",
       "0   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "1   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "2   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "3   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "4   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "5   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "6   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "7   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "8   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "9   bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "10  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "11  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "12  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "13  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "14  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "15  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "16  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "17  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "18  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "19  bigquery-public-data  stackoverflow  stackoverflow_posts   \n",
       "\n",
       "                 column_name                field_path  data_type description  \n",
       "0              comment_count             comment_count      INT64        None  \n",
       "1               post_type_id              post_type_id      INT64        None  \n",
       "2                  parent_id                 parent_id      INT64        None  \n",
       "3                 view_count                view_count      INT64        None  \n",
       "4               answer_count              answer_count      INT64        None  \n",
       "5              owner_user_id             owner_user_id      INT64        None  \n",
       "6                         id                        id      INT64        None  \n",
       "7                      score                     score      INT64        None  \n",
       "8         accepted_answer_id        accepted_answer_id      INT64        None  \n",
       "9             favorite_count            favorite_count      INT64        None  \n",
       "10       last_editor_user_id       last_editor_user_id      INT64        None  \n",
       "11  last_editor_display_name  last_editor_display_name     STRING        None  \n",
       "12        owner_display_name        owner_display_name     STRING        None  \n",
       "13                      body                      body     STRING        None  \n",
       "14                      tags                      tags     STRING        None  \n",
       "15                     title                     title     STRING        None  \n",
       "16      community_owned_date      community_owned_date  TIMESTAMP        None  \n",
       "17        last_activity_date        last_activity_date  TIMESTAMP        None  \n",
       "18            last_edit_date            last_edit_date  TIMESTAMP        None  \n",
       "19             creation_date             creation_date  TIMESTAMP        None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "*\n",
    "FROM\n",
    "`axa-ch-machine-learning-dev.test.schema_stackoverflow`\n",
    "\"\"\"\n",
    "df = client.query(query).to_dataframe()\n",
    "print(len(df))\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preprocessing using Beam/Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ['REGION'] = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# define query table\n",
    "def create_query():\n",
    "    \n",
    "    query = \"\"\"SELECT * FROM `axa-ch-machine-learning-dev.test.schema_stackoverflow` \"\"\"\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to BigQuery! This script will walk you through the \n",
      "process of initializing your .bigqueryrc configuration file.\n",
      "\n",
      "First, we need to set up your credentials if they do not \n",
      "already exist.\n",
      "\n",
      "Credential creation complete. Now we will select a default project.\n",
      "\n",
      "List of projects:\n",
      "[\n",
      "  {\n",
      "    \"#\": 1, \n",
      "    \"friendlyName\": \"machine-learning-poc-dev\", \n",
      "    \"projectId\": \"axa-ch-machine-learning-dev\"\n",
      "  }\n",
      "]\n",
      "Found only one project, setting axa-ch-machine-learning-dev as the default.\n",
      "\n",
      "BigQuery configuration complete! Type \"bq\" to get started.\n",
      "\n",
      "{\n",
      "  \"creationTime\": \"1570200638317\", \n",
      "  \"etag\": \"/Hv4/YKe/f3sCWLopiHN2g==\", \n",
      "  \"id\": \"axa-ch-machine-learning-dev:test.schema_stackoverflow\", \n",
      "  \"kind\": \"bigquery#table\", \n",
      "  \"lastModifiedTime\": \"1570200638317\", \n",
      "  \"location\": \"US\", \n",
      "  \"numBytes\": \"1895\", \n",
      "  \"numLongTermBytes\": \"0\", \n",
      "  \"numRows\": \"20\", \n",
      "  \"schema\": {\n",
      "    \"fields\": [\n",
      "      {\n",
      "        \"mode\": \"NULLABLE\", \n",
      "        \"name\": \"table_catalog\", \n",
      "        \"type\": \"STRING\"\n",
      "      }, \n",
      "      {\n",
      "        \"mode\": \"NULLABLE\", \n",
      "        \"name\": \"table_schema\", \n",
      "        \"type\": \"STRING\"\n",
      "      }, \n",
      "      {\n",
      "        \"mode\": \"NULLABLE\", \n",
      "        \"name\": \"table_name\", \n",
      "        \"type\": \"STRING\"\n",
      "      }, \n",
      "      {\n",
      "        \"mode\": \"NULLABLE\", \n",
      "        \"name\": \"column_name\", \n",
      "        \"type\": \"STRING\"\n",
      "      }, \n",
      "      {\n",
      "        \"mode\": \"NULLABLE\", \n",
      "        \"name\": \"field_path\", \n",
      "        \"type\": \"STRING\"\n",
      "      }, \n",
      "      {\n",
      "        \"mode\": \"NULLABLE\", \n",
      "        \"name\": \"data_type\", \n",
      "        \"type\": \"STRING\"\n",
      "      }, \n",
      "      {\n",
      "        \"mode\": \"NULLABLE\", \n",
      "        \"name\": \"description\", \n",
      "        \"type\": \"STRING\"\n",
      "      }\n",
      "    ]\n",
      "  }, \n",
      "  \"selfLink\": \"https://bigquery.googleapis.com/bigquery/v2/projects/axa-ch-machine-learning-dev/datasets/test/tables/schema_stackoverflow\", \n",
      "  \"tableReference\": {\n",
      "    \"datasetId\": \"test\", \n",
      "    \"projectId\": \"axa-ch-machine-learning-dev\", \n",
      "    \"tableId\": \"schema_stackoverflow\"\n",
      "  }, \n",
      "  \"type\": \"TABLE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# getting schema\n",
    "! bq show --format=prettyjson axa-ch-machine-learning-dev:test.schema_stackoverflow  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "table_schema ={\"fields\": [\n",
    "      {\n",
    "        \"mode\": \"NULLABLE\", \n",
    "        \"name\": \"table_catalog\", \n",
    "        \"type\": \"STRING\"\n",
    "      }, \n",
    "      {\n",
    "        \"mode\": \"NULLABLE\", \n",
    "        \"name\": \"table_schema\", \n",
    "        \"type\": \"STRING\"\n",
    "      }, \n",
    "      {\n",
    "        \"mode\": \"NULLABLE\", \n",
    "        \"name\": \"table_name\", \n",
    "        \"type\": \"STRING\"\n",
    "      }, \n",
    "      {\n",
    "        \"mode\": \"NULLABLE\", \n",
    "        \"name\": \"column_name\", \n",
    "        \"type\": \"STRING\"\n",
    "      }, \n",
    "      {\n",
    "        \"mode\": \"NULLABLE\", \n",
    "        \"name\": \"field_path\", \n",
    "        \"type\": \"STRING\"\n",
    "      }, \n",
    "      {\n",
    "        \"mode\": \"NULLABLE\", \n",
    "        \"name\": \"data_type\", \n",
    "        \"type\": \"STRING\"\n",
    "      }, \n",
    "      {\n",
    "        \"mode\": \"NULLABLE\", \n",
    "        \"name\": \"description\", \n",
    "        \"type\": \"STRING\"\n",
    "      }\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def preprocess(RUNNER):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        -RUNNER: \"DirectRunner\" or \"DataflowRunner\". Specfy to run the pipeline\n",
    "        locally or on Google Cloud respectively. \n",
    "    Side-effects:\n",
    "        -Creates and executes dataflow pipeline. \n",
    "        See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "    \"\"\"\n",
    "    job_name = \"preprocess-mnist\" + \"-\" + datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    print(\"Launching Dataflow job {} ... hang on\".format(job_name))\n",
    "    OUTPUT_DIR = \"gs://{0}/project/stackoverflow/\".format(os.environ['BUCKET_NAME'])\n",
    "\n",
    "    # dictionary of pipeline options\n",
    "    options = {\n",
    "        \"staging_location\": os.path.join(OUTPUT_DIR, \"tmp\", \"staging\"),\n",
    "        \"temp_location\": os.path.join(OUTPUT_DIR, \"tmp\"),\n",
    "        \"job_name\": job_name,\n",
    "        \"project\": os.environ['PROJECT_ID'],\n",
    "        \"runner\": RUNNER,\n",
    "        \"region\": os.environ['REGION'] \n",
    "    }\n",
    "  \n",
    "    # instantiate PipelineOptions object using options dictionary\n",
    "    opts = beam.pipeline.PipelineOptions(flags = [], **options)\n",
    "\n",
    "    #instantantiate Pipeline object using PipelineOptions\n",
    "    with beam.Pipeline(options=opts) as p:\n",
    "            (\n",
    "                p | \"Read from BigQuery\" >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                    # query\n",
    "                    query = create_query(),\n",
    "                    # use standard SQL for the above query\n",
    "                    use_standard_sql = True)\n",
    "                                                        )\n",
    "                  #| 'Write to BigQuery' >>  beam.io.WriteToBigQuery(\n",
    "                  #    # The table name is a required argument for the BigQuery\n",
    "                  #    table='schema_stackoverflow_beam',\n",
    "                  #    dataset='test',\n",
    "                  #    project=os.environ['PROJECT_ID'],\n",
    "                  #    # Here we use the JSON schema read in from a JSON file.\n",
    "                  #    # Specifying the schema allows the API to create the table correctly if it does not yet exist.\n",
    "                  #    schema=table_schema,\n",
    "                  #    # Creates the table in BigQuery if it does not yet exist.\n",
    "                  #    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                  #    # Deletes all data in the BigQuery table before writing.\n",
    "                  #    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n",
    "            )\n",
    "            #p.run().wait_until_finish()\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-mnist-191011-152650 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Retry with exponential backoff: waiting for 3.2655240463532245 seconds before retrying get_query_location because we caught exception: IndexError: tuple index out of range\n",
      " Traceback for above exception (most recent call last):\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\utils\\retry.py\", line 206, in wrapper\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery_tools.py\", line 261, in get_query_location\n",
      "    response = self.client.jobs.Insert(request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\internal\\clients\\bigquery\\bigquery_v2_client.py\", line 342, in Insert\n",
      "    upload=upload, upload_config=upload_config)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\base_api.py\", line 729, in _RunMethod\n",
      "    http, http_request, **opts)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 356, in MakeRequest\n",
      "    max_retry_wait, total_wait_sec))\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 304, in HandleExceptionsAndRebuildHttpConnections\n",
      "    raise retry_args.exc\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 346, in MakeRequest\n",
      "    check_response_func=check_response_func)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 396, in _MakeRequestNoRetry\n",
      "    redirections=redirections, connection_type=connection_type)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\transport.py\", line 153, in new_request\n",
      "    credentials._refresh(orig_request_method)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 765, in _refresh\n",
      "    self._do_refresh_request(http_request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 797, in _do_refresh_request\n",
      "    self.token_uri, method='POST', body=body, headers=headers)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1924, in request\n",
      "    cachekey,\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1595, in _request\n",
      "    conn, request_uri, method, body, headers\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1511, in _conn_request\n",
      "    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n",
      "\n",
      "WARNING:root:Retry with exponential backoff: waiting for 9.111644626003113 seconds before retrying get_query_location because we caught exception: IndexError: tuple index out of range\n",
      " Traceback for above exception (most recent call last):\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\utils\\retry.py\", line 206, in wrapper\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery_tools.py\", line 261, in get_query_location\n",
      "    response = self.client.jobs.Insert(request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\internal\\clients\\bigquery\\bigquery_v2_client.py\", line 342, in Insert\n",
      "    upload=upload, upload_config=upload_config)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\base_api.py\", line 729, in _RunMethod\n",
      "    http, http_request, **opts)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 356, in MakeRequest\n",
      "    max_retry_wait, total_wait_sec))\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 304, in HandleExceptionsAndRebuildHttpConnections\n",
      "    raise retry_args.exc\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 346, in MakeRequest\n",
      "    check_response_func=check_response_func)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 396, in _MakeRequestNoRetry\n",
      "    redirections=redirections, connection_type=connection_type)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\transport.py\", line 153, in new_request\n",
      "    credentials._refresh(orig_request_method)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 765, in _refresh\n",
      "    self._do_refresh_request(http_request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 797, in _do_refresh_request\n",
      "    self.token_uri, method='POST', body=body, headers=headers)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1924, in request\n",
      "    cachekey,\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1595, in _request\n",
      "    conn, request_uri, method, body, headers\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1511, in _conn_request\n",
      "    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n",
      "\n",
      "WARNING:root:Retry with exponential backoff: waiting for 15.553642460644244 seconds before retrying get_query_location because we caught exception: IndexError: tuple index out of range\n",
      " Traceback for above exception (most recent call last):\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\utils\\retry.py\", line 206, in wrapper\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery_tools.py\", line 261, in get_query_location\n",
      "    response = self.client.jobs.Insert(request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\internal\\clients\\bigquery\\bigquery_v2_client.py\", line 342, in Insert\n",
      "    upload=upload, upload_config=upload_config)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\base_api.py\", line 729, in _RunMethod\n",
      "    http, http_request, **opts)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 356, in MakeRequest\n",
      "    max_retry_wait, total_wait_sec))\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 304, in HandleExceptionsAndRebuildHttpConnections\n",
      "    raise retry_args.exc\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 346, in MakeRequest\n",
      "    check_response_func=check_response_func)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 396, in _MakeRequestNoRetry\n",
      "    redirections=redirections, connection_type=connection_type)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\transport.py\", line 153, in new_request\n",
      "    credentials._refresh(orig_request_method)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 765, in _refresh\n",
      "    self._do_refresh_request(http_request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 797, in _do_refresh_request\n",
      "    self.token_uri, method='POST', body=body, headers=headers)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1924, in request\n",
      "    cachekey,\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1595, in _request\n",
      "    conn, request_uri, method, body, headers\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1511, in _conn_request\n",
      "    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n",
      "\n",
      "ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x00000231789A7828>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\utils\\retry.py\", line 216, in wrapper\n",
      "    sleep_interval = next(retry_intervals)\n",
      "StopIteration\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\runners\\direct\\executor.py\", line 343, in call\n",
      "    finish_state)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\runners\\direct\\executor.py\", line 383, in attempt_call\n",
      "    result = evaluator.finish_bundle()\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\runners\\direct\\transform_evaluator.py\", line 318, in finish_bundle\n",
      "    with self._source.reader() as reader:\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery_tools.py\", line 914, in __enter__\n",
      "    self.executing_project, location=self._get_source_location())\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery_tools.py\", line 909, in _get_source_location\n",
      "    self.source.use_legacy_sql)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\utils\\retry.py\", line 219, in wrapper\n",
      "    raise_with_traceback(exn, exn_traceback)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\future\\utils\\__init__.py\", line 421, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\utils\\retry.py\", line 206, in wrapper\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery_tools.py\", line 261, in get_query_location\n",
      "    response = self.client.jobs.Insert(request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\internal\\clients\\bigquery\\bigquery_v2_client.py\", line 342, in Insert\n",
      "    upload=upload, upload_config=upload_config)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\base_api.py\", line 729, in _RunMethod\n",
      "    http, http_request, **opts)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 356, in MakeRequest\n",
      "    max_retry_wait, total_wait_sec))\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 304, in HandleExceptionsAndRebuildHttpConnections\n",
      "    raise retry_args.exc\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 346, in MakeRequest\n",
      "    check_response_func=check_response_func)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 396, in _MakeRequestNoRetry\n",
      "    redirections=redirections, connection_type=connection_type)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\transport.py\", line 153, in new_request\n",
      "    credentials._refresh(orig_request_method)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 765, in _refresh\n",
      "    self._do_refresh_request(http_request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 797, in _do_refresh_request\n",
      "    self.token_uri, method='POST', body=body, headers=headers)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1924, in request\n",
      "    cachekey,\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1595, in _request\n",
      "    conn, request_uri, method, body, headers\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1511, in _conn_request\n",
      "    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "WARNING:root:Retry with exponential backoff: waiting for 3.925964170416567 seconds before retrying get_query_location because we caught exception: IndexError: tuple index out of range\n",
      " Traceback for above exception (most recent call last):\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\utils\\retry.py\", line 206, in wrapper\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery_tools.py\", line 261, in get_query_location\n",
      "    response = self.client.jobs.Insert(request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\internal\\clients\\bigquery\\bigquery_v2_client.py\", line 342, in Insert\n",
      "    upload=upload, upload_config=upload_config)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\base_api.py\", line 729, in _RunMethod\n",
      "    http, http_request, **opts)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 356, in MakeRequest\n",
      "    max_retry_wait, total_wait_sec))\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 304, in HandleExceptionsAndRebuildHttpConnections\n",
      "    raise retry_args.exc\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 346, in MakeRequest\n",
      "    check_response_func=check_response_func)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 396, in _MakeRequestNoRetry\n",
      "    redirections=redirections, connection_type=connection_type)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\transport.py\", line 153, in new_request\n",
      "    credentials._refresh(orig_request_method)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 765, in _refresh\n",
      "    self._do_refresh_request(http_request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 797, in _do_refresh_request\n",
      "    self.token_uri, method='POST', body=body, headers=headers)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1924, in request\n",
      "    cachekey,\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1595, in _request\n",
      "    conn, request_uri, method, body, headers\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1511, in _conn_request\n",
      "    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n",
      "\n",
      "WARNING:root:Retry with exponential backoff: waiting for 5.086962822351405 seconds before retrying get_query_location because we caught exception: IndexError: tuple index out of range\n",
      " Traceback for above exception (most recent call last):\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\utils\\retry.py\", line 206, in wrapper\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery_tools.py\", line 261, in get_query_location\n",
      "    response = self.client.jobs.Insert(request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apache_beam\\io\\gcp\\internal\\clients\\bigquery\\bigquery_v2_client.py\", line 342, in Insert\n",
      "    upload=upload, upload_config=upload_config)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\base_api.py\", line 729, in _RunMethod\n",
      "    http, http_request, **opts)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 356, in MakeRequest\n",
      "    max_retry_wait, total_wait_sec))\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 304, in HandleExceptionsAndRebuildHttpConnections\n",
      "    raise retry_args.exc\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 346, in MakeRequest\n",
      "    check_response_func=check_response_func)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\apitools\\base\\py\\http_wrapper.py\", line 396, in _MakeRequestNoRetry\n",
      "    redirections=redirections, connection_type=connection_type)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\transport.py\", line 153, in new_request\n",
      "    credentials._refresh(orig_request_method)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 765, in _refresh\n",
      "    self._do_refresh_request(http_request)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\oauth2client\\client.py\", line 797, in _do_refresh_request\n",
      "    self.token_uri, method='POST', body=body, headers=headers)\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1924, in request\n",
      "    cachekey,\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1595, in _request\n",
      "    conn, request_uri, method, body, headers\n",
      "  File \"C:\\Users\\c311723\\.conda\\envs\\env_data_loss_prev\\lib\\site-packages\\httplib2\\__init__.py\", line 1511, in _conn_request\n",
      "    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess(\"DirectRunner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-env_data_loss_prev]",
   "language": "python",
   "name": "conda-env-.conda-env_data_loss_prev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
